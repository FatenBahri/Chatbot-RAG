{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f0f6d44-fed3-4f96-a73d-0eb730216a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import psycopg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f787edeb-5a2c-469f-8d43-9ad6c22bf4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Mod√®le d'embedding : nomic-embed-text\n",
      "‚úì Mod√®le de chat : gpt-oss:120b-cloud\n",
      "‚úì Base de donn√©es : PostgreSQL sous wsl (port 5433)\n"
     ]
    }
   ],
   "source": [
    "db_connection_str = \"postgresql://postgres:postgres@localhost:5433/chatbot\"\n",
    "\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"  # Mod√®le pour les embeddings\n",
    "CHAT_MODEL = \"gpt-oss:120b-cloud\"       # Mod√®le pour la g√©n√©ration de r√©ponses\n",
    "\n",
    "print(f\"‚úì Mod√®le d'embedding : {EMBEDDING_MODEL}\")\n",
    "print(f\"‚úì Mod√®le de chat : {CHAT_MODEL}\")\n",
    "print(f\"‚úì Base de donn√©es : PostgreSQL sous wsl (port 5433)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c711eff6-7eb7-4891-a45e-1d8984a5d27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fonctions d√©finies\n"
     ]
    }
   ],
   "source": [
    "def embed_text(text: str, model_name: str = EMBEDDING_MODEL) -> list[float]:\n",
    "    \"\"\"\n",
    "    G√©n√®re un embedding pour le texte donn√© avec Ollama\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        raise ValueError(\"Le texte ne peut pas √™tre vide\")\n",
    "    \n",
    "    response = ollama.embeddings(\n",
    "        model=model_name,\n",
    "        prompt=text\n",
    "    )\n",
    "    return response[\"embedding\"]\n",
    "\n",
    "def fetch_similar_from_db(query_embedding: list[float], top_k: int = 3, connection: str = db_connection_str):\n",
    "    \"\"\"\n",
    "    R√©cup√®re les top-k documents les plus similaires depuis la base de donn√©es\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples (id, corpus, similarity)\n",
    "    \"\"\"\n",
    "    if not query_embedding:\n",
    "        return []\n",
    "    \n",
    "    with psycopg.connect(connection) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                SELECT id, corpus,\n",
    "                       1 - (embedding <=> %s::vector) AS similarity\n",
    "                FROM embeddings\n",
    "                ORDER BY embedding <=> %s::vector\n",
    "                LIMIT %s\n",
    "                \"\"\",\n",
    "                (query_embedding, query_embedding, top_k),\n",
    "            )\n",
    "            return cur.fetchall()\n",
    "\n",
    "\n",
    "def build_prompt(question: str, docs: list[tuple], max_chars: int = 2000) -> str:\n",
    "    \"\"\"\n",
    "    Construit un prompt RAG avec le contexte et la question\n",
    "    Returns:\n",
    "        Prompt format√© pour le mod√®le\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    used = 0\n",
    "    \n",
    "    for _id, content, _sim in docs:\n",
    "        if not content:\n",
    "            continue\n",
    "        part = content.strip()\n",
    "        if used + len(part) > max_chars:\n",
    "            part = part[: max(0, max_chars - used)]\n",
    "        context_parts.append(part)\n",
    "        used += len(part)\n",
    "        if used >= max_chars:\n",
    "            break\n",
    "    \n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    \n",
    "    prompt = (\n",
    "        \"Tu es un assistant . Utilise UNIQUEMENT les documents fournis ci-dessous pour r√©pondre √† la question.\\n\\n\"\n",
    "        \"Si la r√©ponse n'est pas dans les documents, dis 'Je ne sais pas'. \\n\\n\"\n",
    "        \"CONTEXTE:\\n\" + context_text + \"\\n\\n\"\n",
    "        \"QUESTION: \" + question + \"\\n\\n\"\n",
    "        \"R√©ponds de mani√®re claire et concise en te basant UNIQUEMENT sur le contexte fourni:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_answer(question: str, top_k: int = 3, chat_model: str = CHAT_MODEL, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    Fonction principale : g√©n√®re une r√©ponse RAG compl√®te\n",
    "    \n",
    "    Args:\n",
    "        question: Question de l'utilisateur\n",
    "        top_k: Nombre de documents √† r√©cup√©rer\n",
    "        chat_model: Mod√®le Ollama √† utiliser\n",
    "        verbose: Afficher les d√©tails du prompt\n",
    "    \n",
    "    Returns:\n",
    "        dict avec 'answer', 'contexts' et 'prompt'\n",
    "    \"\"\"\n",
    "    # 1. Calculer l'embedding de la question\n",
    "    print(f\"üîç Recherche de documents similaires...\")\n",
    "    q_emb = embed_text(question)\n",
    "    \n",
    "    # 2. R√©cup√©rer les documents similaires\n",
    "    docs = fetch_similar_from_db(q_emb, top_k=top_k)\n",
    "    \n",
    "    if not docs:\n",
    "        return {\n",
    "            \"answer\": \"Aucun document pertinent trouv√© dans la base de donn√©es.\",\n",
    "            \"contexts\": [],\n",
    "            \"prompt\": \"\"\n",
    "        }\n",
    "    \n",
    "    print(f\"‚úì {len(docs)} document(s) trouv√©(s)\")\n",
    "    \n",
    "    # 3. Construire le prompt\n",
    "    prompt = build_prompt(question, docs)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüìù Prompt g√©n√©r√© ({len(prompt)} caract√®res):\\n\")\n",
    "        print(\"=\" * 70)\n",
    "        print(prompt)\n",
    "        print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    # 4. G√©n√©rer la r√©ponse avec Ollama\n",
    "    print(f\"ü§ñ G√©n√©ration de la r√©ponse avec {chat_model}...\")\n",
    "    \n",
    "    response = ollama.generate(\n",
    "        model=chat_model,\n",
    "        prompt=prompt,\n",
    "        options={\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.9,\n",
    "            'num_predict': 500,  # Nombre maximum de tokens √† g√©n√©rer\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    answer = response['response']\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": docs,\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "\n",
    "print(\"‚úì Fonctions d√©finies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9c9c9-2cfc-477a-9fd9-874ecc5cc892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : Quels sont les offre de travail disponibles ?\n",
      "\n",
      "üîç Recherche de documents similaires...\n"
     ]
    }
   ],
   "source": [
    "test_question = \"Quels sont les offre de travail disponibles ?\"\n",
    "\n",
    "print(f\"Question : {test_question}\\n\")\n",
    "\n",
    "result = generate_answer(test_question, top_k=3, verbose=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìù R√âPONSE\")\n",
    "print(\"=\" * 70)\n",
    "print(result[\"answer\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìö SOURCES UTILIS√âES\")\n",
    "print(\"=\" * 70)\n",
    "for i, (_id, text, sim) in enumerate(result[\"contexts\"], 1):\n",
    "    preview = text[:150] + \"...\" if len(text) > 150 else text\n",
    "    print(f\"\\n[{i}] ID:{_id} | Similarit√©: {sim:.4f}\")\n",
    "    print(f\"    {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15636964-2a71-455d-b76d-a7772b536e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : combien dappeles sont faites ?\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m test_question = \u001b[33m\"\u001b[39m\u001b[33mcombien dappeles sont faites ?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuestion : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m result = \u001b[43mgenerate_answer\u001b[49m(test_question, top_k=\u001b[32m3\u001b[39m, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìù R√âPONSE\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_answer' is not defined"
     ]
    }
   ],
   "source": [
    "test_question = \"combien dappeles sont faites ?\"\n",
    "\n",
    "print(f\"Question : {test_question}\\n\")\n",
    "\n",
    "result = generate_answer(test_question, top_k=3, verbose=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìù R√âPONSE\")\n",
    "print(\"=\" * 70)\n",
    "print(result[\"answer\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìö SOURCES UTILIS√âES\")\n",
    "print(\"=\" * 70)\n",
    "for i, (_id, text, sim) in enumerate(result[\"contexts\"], 1):\n",
    "    preview = text[:150] + \"...\" if len(text) > 150 else text\n",
    "    print(f\"\\n[{i}] ID:{_id} | Similarit√©: {sim:.4f}\")\n",
    "    print(f\"    {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de18ed82-834f-49dd-a927-ec3da6faba19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

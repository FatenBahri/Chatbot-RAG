{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c4f7313-3a54-40a4-a1c3-dc665d800b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\bahri\\onedrive\\bureau\\chabot-old\\chatbot-rag\\venv\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\bahri\\onedrive\\bureau\\chabot-old\\chatbot-rag\\venv\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\bahri\\onedrive\\bureau\\chabot-old\\chatbot-rag\\venv\\lib\\site-packages (from ollama) (2.12.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\bahri\\onedrive\\bureau\\chabot-old\\chatbot-rag\\venv\\lib\\site-packages (from httpx>=0.27->ollama) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\bahri\\onedrive\\bureau\\chabot-old\\chatbot-rag\\venv\\lib\\site-packages (from httpx>=0.27->ollama) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\bahri\\onedrive\\bureau\\chabot-old\\chatbot-rag\\venv\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\bahri\\onedrive\\bureau\\chabot-old\\chatbot-rag\\venv\\lib\\site-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\bahri\\onedrive\\bureau\\chabot-old\\chatbot-rag\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\bahri\\onedrive\\bureau\\chabot-old\\chatbot-rag\\venv\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\bahri\\onedrive\\bureau\\chabot-old\\chatbot-rag\\venv\\lib\\site-packages (from pydantic>=2.9->ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\bahri\\onedrive\\bureau\\chabot-old\\chatbot-rag\\venv\\lib\\site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\bahri\\onedrive\\bureau\\chabot-old\\chatbot-rag\\venv\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd8538c-875d-4c2f-81cc-49f91f5647d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9bd5afd-6a44-4ecc-9b87-09dfc49da36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "from psycopg import Cursor\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35518441-bdd0-4795-9ec8-eead76225575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dossier de donn√©es : C:\\Users\\bahri\\OneDrive\\Bureau\\chabot-old\\Chatbot-RAG\\clone\\Chatbot-RAG\\data\n",
      "‚úì Mod√®le Ollama : nomic-embed-text\n",
      "‚úì Configuration charg√©e\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(os.path.dirname(os.getcwd()), \"data\")\n",
    "\n",
    "OLLAMA_MODEL = \"nomic-embed-text\"\n",
    "\n",
    "db_connection_str = \"postgresql://postgres:postgres@localhost:5433/chatbot\"\n",
    "\n",
    "print(f\"‚úì Dossier de donn√©es : {data_dir}\")\n",
    "print(f\"‚úì Mod√®le Ollama : {OLLAMA_MODEL}\")\n",
    "print(f\"‚úì Configuration charg√©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd6d3b2-6d88-4c85-b39d-ae1f17966aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Mod√®le nomic-embed-text non trouv√©\n",
      "   Mod√®les disponibles : \n",
      "\n",
      "üß™ Test d'embedding...\n",
      "‚úì Ollama fonctionne correctement\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    models = ollama.list()\n",
    "    \n",
    "    # Extraire correctement tous les noms disponibles\n",
    "    if isinstance(models, dict) and 'models' in models:\n",
    "        model_names = [m.get('name') or m.get('model') for m in models['models']]\n",
    "    elif isinstance(models, list):\n",
    "        model_names = [m.get('name') or m.get('model') for m in models]\n",
    "    else:\n",
    "        model_names = []\n",
    "\n",
    "    if OLLAMA_MODEL in model_names:\n",
    "        print(f\"‚úì Mod√®le {OLLAMA_MODEL} disponible\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Mod√®le {OLLAMA_MODEL} non trouv√©\")\n",
    "        print(\"   Mod√®les disponibles :\", \", \".join(model_names))\n",
    "\n",
    "    # Test embeddings\n",
    "    print(\"\\nüß™ Test d'embedding...\")\n",
    "    test_response = ollama.embeddings(model=OLLAMA_MODEL, prompt=\"test\")\n",
    "    print(f\"‚úì Ollama fonctionne correctement\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de connexion √† Ollama : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f6026ae-d07e-4c49-90a1-3574084d6718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fonctions d√©finies\n"
     ]
    }
   ],
   "source": [
    "def create_conversation_list(file_path: str) -> list[str]:\n",
    "    \"\"\"Lit le fichier avec le bon encodage et filtre les lignes\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"‚ö†Ô∏è  Encodage UTF-8 invalide pour {file_path} ‚Äî r√©essayage avec cp1252\")\n",
    "        with open(file_path, \"r\", encoding=\"cp1252\", errors=\"replace\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "    text_list = text.split(\"\\n\")\n",
    "    filtered_list = [\n",
    "        chaine.removeprefix(\"     \")\n",
    "        for chaine in text_list\n",
    "        if not chaine.startswith(\"<\") and chaine.strip()\n",
    "    ]\n",
    "    print(f\"‚úì {len(filtered_list)} lignes extraites\")\n",
    "    return filtered_list\n",
    "\n",
    "\n",
    "def calculate_embeddings(corpus: str) -> list[float]:\n",
    "    \"\"\"Calcule les embeddings avec Ollama\"\"\"\n",
    "    if not corpus or not corpus.strip():\n",
    "        raise ValueError(\"Le corpus ne peut pas √™tre vide\")\n",
    "    \n",
    "    # Appel √† Ollama pour g√©n√©rer l'embedding\n",
    "    response = ollama.embeddings(\n",
    "        model=OLLAMA_MODEL,\n",
    "        prompt=corpus\n",
    "    )\n",
    "    return response[\"embedding\"]\n",
    "\n",
    "\n",
    "def save_embedding(corpus: str, embedding: list[float], cursor: Cursor) -> None:\n",
    "    \"\"\"Sauvegarde le corpus et son embedding\"\"\"\n",
    "    cursor.execute(\n",
    "        '''INSERT INTO embeddings (corpus, embedding) VALUES (%s, %s)''',\n",
    "        (corpus, embedding)\n",
    "    )\n",
    "\n",
    "\n",
    "def similar_corpus(input_corpus: str, db_connection_str: str, top_k: int = 5) -> list[tuple]:\n",
    "    \"\"\"\n",
    "    Recherche les textes similaires dans la base de donn√©es\n",
    "    \"\"\"\n",
    "    query_embedding = calculate_embeddings(input_corpus)\n",
    "    \n",
    "    with psycopg.connect(db_connection_str) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT id, corpus, \n",
    "                       1 - (embedding <=> %s::vector) as similarity\n",
    "                FROM embeddings\n",
    "                ORDER BY embedding <=> %s::vector\n",
    "                LIMIT %s\n",
    "            \"\"\", (query_embedding, query_embedding, top_k))\n",
    "            \n",
    "            return cur.fetchall()\n",
    "\n",
    "print(\"‚úì Fonctions d√©finies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc10c4e-9d68-417c-8da8-fe36d288452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_text = \"bonjour tous le monde\"\n",
    "    test_embedding = calculate_embeddings(test_text)\n",
    "    print(f\"‚úì Test r√©ussi !\")\n",
    "    print(f\"  Texte : '{test_text}'\")\n",
    "    print(f\"  Dimension de l'embedding : {len(test_embedding)}\")\n",
    "    print(f\"  Premiers valeurs : {test_embedding[:5]}\")\n",
    "    \n",
    "    # Stocker la dimension pour la cr√©ation de table\n",
    "    EMBEDDING_DIM = len(test_embedding)\n",
    "    print(f\"\\n‚úì Dimension d√©tect√©e : {EMBEDDING_DIM}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors du test : {e}\")\n",
    "    EMBEDDING_DIM = 768  # Valeur par d√©faut pour gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3008730-8392-4ea5-bce1-7aa9f23f3626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ CR√âATION DE LA BASE D'EMBEDDINGS\n",
      "======================================================================\n",
      "‚úì Index existant supprim√©\n",
      "‚úì Table existante supprim√©e\n",
      "‚úì Extension pgvector cr√©√©e\n",
      "‚úì Table embeddings cr√©√©e avec VECTOR(768)\n",
      "‚úì Table pr√™te (l'index sera cr√©√© apr√®s insertion)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"üöÄ CR√âATION DE LA BASE D'EMBEDDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with psycopg.connect(db_connection_str) as conn:\n",
    "    conn.autocommit = True\n",
    "    with conn.cursor() as cur:\n",
    "        # Supprimer l'index d'abord (pour √©viter les erreurs)\n",
    "        cur.execute(\"\"\"DROP INDEX IF EXISTS embeddings_embedding_idx\"\"\")\n",
    "        print(\"‚úì Index existant supprim√©\")\n",
    "        \n",
    "        # Supprimer la table si elle existe\n",
    "        cur.execute(\"\"\"DROP TABLE IF EXISTS embeddings CASCADE\"\"\")\n",
    "        print(\"‚úì Table existante supprim√©e\")\n",
    "        \n",
    "        # Cr√©er l'extension pgvector\n",
    "        cur.execute(\"\"\"CREATE EXTENSION IF NOT EXISTS vector\"\"\")\n",
    "        print(\"‚úì Extension pgvector cr√©√©e\")\n",
    "        \n",
    "        # Cr√©er la table avec la dimension d√©tect√©e\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE TABLE embeddings (\n",
    "                id SERIAL PRIMARY KEY, \n",
    "                corpus TEXT,\n",
    "                embedding VECTOR({EMBEDDING_DIM})\n",
    "            )\n",
    "        \"\"\")\n",
    "        print(f\"‚úì Table embeddings cr√©√©e avec VECTOR({EMBEDDING_DIM})\")\n",
    "        \n",
    "        # Cr√©er un index pour acc√©l√©rer les recherches\n",
    "        # Note: L'index sera cr√©√© apr√®s l'insertion des donn√©es pour de meilleures performances\n",
    "        print(\"‚úì Table pr√™te (l'index sera cr√©√© apr√®s insertion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9a3917e-b339-4563-ac1b-411d01d07d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìÇ CHARGEMENT DES DONN√âES\n",
      "======================================================================\n",
      "‚úì 41 fichier(s) trouv√©(s)\n",
      "  1. 017_00000012.txt\n",
      "  2. 018_00000013.txt\n",
      "  3. 019_00000014.txt\n",
      "  4. 020_00000015.txt\n",
      "  5. 022_00000017.txt\n",
      "  ... et 36 autre(s)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìÇ CHARGEMENT DES DONN√âES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "text_files = sorted(glob.glob(os.path.join(data_dir, \"*.txt\")))\n",
    "\n",
    "if not text_files:\n",
    "    print(f\"‚ö†Ô∏è  Aucun fichier .txt trouv√© dans le dossier {data_dir}\")\n",
    "else:\n",
    "    print(f\"‚úì {len(text_files)} fichier(s) trouv√©(s)\")\n",
    "    for i, file in enumerate(text_files[:5], 1):  # Afficher les 5 premiers\n",
    "        print(f\"  {i}. {os.path.basename(file)}\")\n",
    "    if len(text_files) > 5:\n",
    "        print(f\"  ... et {len(text_files) - 5} autre(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b8942-09d5-48ad-97aa-b5ab20647faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚öôÔ∏è  TRAITEMENT DES EMBEDDINGS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚öôÔ∏è  TRAITEMENT DES EMBEDDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "success_count = 0\n",
    "error_count = 0\n",
    "\n",
    "with psycopg.connect(db_connection_str) as conn:\n",
    "    conn.autocommit = True\n",
    "    with conn.cursor() as cur:\n",
    "        total_files = len(text_files)\n",
    "        \n",
    "        for file_idx, file_path in enumerate(text_files, 1):\n",
    "            print(f\"\\nüî∏ Traitement du fichier [{file_idx}/{total_files}] : {os.path.basename(file_path)}\")\n",
    "            corpus_list = create_conversation_list(file_path=file_path)\n",
    "\n",
    "            for i, corpus in enumerate(corpus_list, 1):\n",
    "                try:\n",
    "                    embedding = calculate_embeddings(corpus)\n",
    "                    save_embedding(corpus=corpus, embedding=embedding, cursor=cur)\n",
    "                    success_count += 1\n",
    "                \n",
    "                    # Afficher un aper√ßu\n",
    "                    preview = corpus[:50] + \"...\" if len(corpus) > 50 else corpus\n",
    "                    print(f\"‚úì [{i}/{len(corpus_list)}] {preview}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    error_count += 1\n",
    "                    print(f\"‚úó [{i}/{len(corpus_list)}] ERREUR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b2221b-daec-4ac9-b17b-1afe8f8da884",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä R√âSUM√â\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"‚úì Succ√®s: {success_count}\")\n",
    "print(f\"‚úó Erreurs: {error_count}\")\n",
    "print(f\"üì¶ Total sauvegard√©: {success_count}\")\n",
    "\n",
    "# Cr√©er l'index maintenant que les donn√©es sont ins√©r√©es\n",
    "if success_count > 0:\n",
    "    print(\"\\nüîß Cr√©ation de l'index de recherche...\")\n",
    "    with psycopg.connect(db_connection_str) as conn:\n",
    "        conn.autocommit = True\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE INDEX IF NOT EXISTS embeddings_embedding_idx \n",
    "                ON embeddings USING ivfflat (embedding vector_cosine_ops)\n",
    "                WITH (lists = 100)\n",
    "            \"\"\")\n",
    "            print(\"‚úì Index de recherche cr√©√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a38f61-8107-4497-82a4-67cfa4113d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if success_count > 0:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üîç TEST DE RECHERCHE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    test_query = \"stage informatique\"\n",
    "    print(f\"Requ√™te: '{test_query}'\")\n",
    "    \n",
    "    try:\n",
    "        results = similar_corpus(test_query, db_connection_str, top_k=3)\n",
    "        print(f\"\\nüìå Top 2 r√©sultats:\")\n",
    "        for doc_id, corpus, similarity in results:\n",
    "            preview = corpus[:60] + \"...\" if len(corpus) > 60 else corpus\n",
    "            print(f\"  [Score: {similarity:.4f}] {preview}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7adf6-adf7-425a-bb20-7a7bb486478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recherche_interactive(query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Fonction pour faire des recherches facilement dans le notebook\n",
    "    \"\"\"\n",
    "    print(f\"üîç Recherche: '{query}'\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    try:\n",
    "        results = similar_corpus(query, db_connection_str, top_k=top_k)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"Aucun r√©sultat trouv√©\")\n",
    "            return\n",
    "        \n",
    "        for i, (doc_id, corpus, similarity) in enumerate(results, 1):\n",
    "            print(f\"\\nüìÑ R√©sultat {i} (Score: {similarity:.4f})\")\n",
    "            print(f\"   ID: {doc_id}\")\n",
    "            print(f\"   Texte: {corpus[:100]}{'...' if len(corpus) > 100 else ''}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur: {e}\")\n",
    "\n",
    "# Exemple d'utilisation :\n",
    "# recherche_interactive(\"votre requ√™te ici\", top_k=3)\n",
    "\n",
    "print(\" Fonction de recherche interactive \")\n",
    "print(\"  Utilisation: recherche_interactive('votre requ√™te', top_k=5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb250ad6-686f-4b91-85c0-eaf07003b3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d84e9e-8808-4efe-ad97-0caeb9359e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
